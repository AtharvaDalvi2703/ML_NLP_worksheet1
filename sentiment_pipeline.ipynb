{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%env WANDB_DISABLED=true\n",
        "%pip install wandb"
      ],
      "metadata": {
        "id": "dIsNQHrqQd3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnoGV0KIL7pv"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade transformers datasets\n",
        "import transformers\n",
        "%pip install transformers[torch]\n",
        "%pip install 'accelerate>=0.26.0\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "dataset=load_dataset(\"imdb\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n"
      ],
      "metadata": {
        "id": "RSo5T8pUMB5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(data):\n",
        "  return tokenizer(data[\"text\"], padding=True, truncation=True,max_length=256) #512"
      ],
      "metadata": {
        "id": "z3taNZzJMFlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_text=dataset.map(tokenize,batched=True)"
      ],
      "metadata": {
        "id": "ethn4-i9MGyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate\n"
      ],
      "metadata": {
        "id": "XUafLPfUMLVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "import evaluate\n",
        "model=AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\",num_labels=2)\n",
        "\n"
      ],
      "metadata": {
        "id": "ktqaHDHkMMU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_metric.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
        "        \"f1\":       f1_metric.compute(predictions=preds, references=labels, average=\"binary\")[\"f1\"],\n",
        "    }"
      ],
      "metadata": {
        "id": "H0Iv0lK9MRE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    fp16=True,\n",
        "    num_train_epochs=1,\n",
        "    report_to=[]\n",
        ")\n"
      ],
      "metadata": {
        "id": "1HPVD86BMTNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric       = evaluate.load(\"f1\")\n"
      ],
      "metadata": {
        "id": "HC9vAGabPDVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_train = tokenized_text[\"train\"] \\\n",
        "    .shuffle(seed=42) \\\n",
        "    .select(range(5000))\n",
        "small_test  = tokenized_text[\"test\"] \\\n",
        "    .shuffle(seed=42) \\\n",
        "    .select(range(1000))"
      ],
      "metadata": {
        "id": "SeyB4RfASrSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train,\n",
        "    eval_dataset= small_test,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "-LZ8nqOPPlZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_metrics = trainer.evaluate()\n",
        "print(f\"Eval Accuracy: {eval_metrics['eval_accuracy']:.4f}\")\n",
        "print(f\"Eval F1:       {eval_metrics['eval_f1']:.4f}\")\n"
      ],
      "metadata": {
        "id": "rmP856DiUCQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"fine_tuned_bert_imdb\")\n",
        "tokenizer.save_pretrained(\"fine_tuned_bert_imdb\")"
      ],
      "metadata": {
        "id": "I1zVUIOFU-jS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "\n",
        "sentiment = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=\"fine_tuned_bert_imdb\",\n",
        "    tokenizer=\"fine_tuned_bert_imdb\"\n",
        ")\n",
        "\n",
        "\n",
        "print(sentiment(\"What an amazing film—truly a masterpiece!\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "5QmWj31ZVD8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pipeline Overview-\n",
        "\n",
        "The machine learning pipeline launches by loading the IMDb movie‐review dataset through the Hugging Face datasets library, we have split the data into training and test sets and fixed a random seed for the sake of reproducibility.\n",
        "\n",
        "Components and Rationale\n",
        "Tokenizer-\n",
        "\n",
        "We have used the method AutoTokenizer.from_pretrained(\"bert-base-uncased\") to convert raw text into token IDs. We used padding and truncating with a maximum length of 256 tokens to get a balance between expressive capacity and computational costs.\n",
        "\n",
        "Model-\n",
        "\n",
        "We have used \"AutoModelForSequenceClassification\" (and a num_labels=2 argument) to provide a pre‐trained BERT encoder and lightweight classification head model. The selection of the model takes advantage of transfer learning to generalize better when trained with only a limited amount of observation data.\n",
        "\n",
        "Training & Evaluation-\n",
        "\n",
        "The pipeline also sets the learning rate (2e‑5), batch size (8), and to train one epoch (sufficient for proof of concept while managing GPU runtime). The Trainer handler optimises the model and includes logging & checkpointing capabilities. The pipeline's evaluate library also permits accuracy and F1‑score (binary average) to be computed.\n",
        "\n",
        "Inference-\n",
        "\n",
        "once the fine‑tuned model and tokenizer are saved we are able to wrap them in a Hugging Face pipeline for end‑to‑end text‑classification, providing ease of deployment and allowing rapid prototype capabilities.\n",
        "\n",
        "Challenges faced-\n",
        "\n",
        "The major challenge faced is the training phase, the training took significant time, and transformer also gave some issues in training arguments, so install some required libraries.\n",
        "To make the training process fast I used small portion of the training and test dataset.\n",
        "Anyway the worksheet gave me a great fun."
      ],
      "metadata": {
        "id": "bM6-HJ3YePCm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x5o6l13VrnBu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}